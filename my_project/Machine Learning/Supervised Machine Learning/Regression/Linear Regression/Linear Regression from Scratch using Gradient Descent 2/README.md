# Linear Regression from Scratch using Gradient Descent

## Overview
This project demonstrates how **Linear Regression** works internally by implementing **Gradient Descent from scratch** using NumPy.  
Synthetic data is generated to clearly visualize coefficient learning, error minimization, and model convergence.

---

## Objective
- Understand linear regression without using Scikit-learn  
- Implement batch gradient descent manually  
- Visualize loss convergence and error surfaces  

---

## Dataset
- **Synthetic dataset** generated using NumPy  
- Features:
  - Feature1  
  - Feature2  
- Target:
  - Linear combination of features with added noise  

---

## Methodology
- Generated data using known true coefficients  
- Added bias term explicitly  
- Used Mean Squared Error (MSE) as loss  
- Updated parameters using gradient descent  

---

## Visualization
- Data scatter plot with true regression line  
- Loss vs iterations (convergence curve)  
- 2D contour plot of error surface  
- 3D surface plot of Mean Squared Error  

---

## Key Insight
- Gradient descent converges to a global minimum  
- Proper learning rate ensures smooth optimization  
- Visualizations help understand optimization dynamics  

---

## Tools Used
- Python  
- NumPy  
- Pandas  
- Matplotlib  

---

## Author
**Neel Arora**  
BCA Undergraduate | AI & ML  

---

## Notes
This project is intended for learning and conceptual understanding of gradient descent.
